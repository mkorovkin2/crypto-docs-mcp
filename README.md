# Crypto Documentation MCP Server

A Model Context Protocol (MCP) server that provides blockchain developer documentation to AI coding agents like Claude Code and Cursor. Uses LLM-powered answer synthesis with an agentic evaluation loop to deliver comprehensive responses with code examples, imports, and source citations. Supports multiple LLM providers (OpenAI, Anthropic, XAI, AWS Bedrock) and 7 crypto/DeFi projects: Mina Protocol, Solana, Cosmos SDK, Secret Network, Beam, Pirate Chain, and Polymarket.

## Features

- **LLM-Synthesized Answers**: Get comprehensive answers generated by your choice of LLM (GPT-4o, Claude, Grok, Bedrock models), not just raw documentation chunks
- **Multi-Provider LLM Support**: Choose from OpenAI, Anthropic (Claude), XAI (Grok), or AWS Bedrock with per-purpose configuration (synthesis, evaluation, refinement)
- **Agentic Evaluation Loop**: Iterative self-improving answers with optional Tavily web search integration when documentation is insufficient
- **Multi-Project Support**: Query documentation from 7 projects: Mina, Solana, Cosmos, Secret Network, Beam, Pirate Chain, and Polymarket
- **Hybrid Search**: Combines vector similarity (Qdrant) and full-text search (SQLite FTS5) with Reciprocal Rank Fusion
- **Smart Reranking**: Results are reranked using a fast LLM for higher relevance
- **Corrective RAG**: Automatically retries with alternative queries when initial results have low confidence
- **Working Code Examples**: Get complete, runnable code with imports and setup instructions
- **Error Debugging**: Explain errors with root cause analysis and fixes
- **Source Citations**: All answers include links to original documentation sources
- **Intelligent GitHub Scraping**: Quality-filtered indexing of GitHub examples with LLM relevance scoring
- **Trust Levels**: Tiered source trust (official > verified-community > community) affects search ranking
- **Source Registry**: Modular configuration for multiple GitHub repos per project
- **Evaluation Suite**: Comprehensive test harness with LLM-based validation for quality assurance

## Supported Projects

| Project | ID | Documentation | Source Code |
|---------|-----|--------------|-------------|
| Mina Protocol | `mina` | docs.minaprotocol.com | o1-labs/o1js, o1-labs/zkapp-cli |
| Solana | `solana` | solana.com/docs | solana-labs/solana-program-library |
| Cosmos SDK | `cosmos` | docs.cosmos.network | cosmos/cosmos-sdk |
| Secret Network | `secret` | docs.scrt.network | scrtlabs/SecretNetwork, scrtlabs/secret.js, scrtlabs/secret-toolkit |
| Beam | `beam` | github.com/BeamMW/beam/wiki | BeamMW/beam, BeamMW/web-wallet, BeamMW/shader-sdk |
| Pirate Chain | `pirate-chain` | docs.piratechain.com | PirateNetwork/pirate, PirateNetwork/lightwalletd |
| Polymarket | `polymarket` | docs.polymarket.com | Polymarket/py-clob-client, Polymarket/clob-client |

### Quick Index Commands

```bash
# Index all projects (recommended with --use-registry for GitHub quality filtering)
npm run scraper -- -p mina --use-registry
npm run scraper -- -p solana --use-registry
npm run scraper -- -p cosmos --use-registry
npm run scraper -- -p secret --use-registry
npm run scraper -- -p beam --use-registry
npm run scraper -- -p pirate-chain --use-registry
```

Add more projects by creating a configuration file in `config/projects/`.

### Index Generated Repository Docs

Index locally generated documentation (e.g. from `/generate_repo_docs`) under an existing project:

```bash
npm run index:generated-docs -- --project polymarket --custom Polymarket-Kalshi-Arbitrage-bot
# Or point directly at a folder:
npm run index:generated-docs -- --project polymarket --dir docs/generated/Polymarket-Kalshi-Arbitrage-bot
```

For the existing generated docs in this repo, run:

```bash
npm run index:generated-docs -- --project polymarket --dir docs/generated/Polymarket-Kalshi-Arbitrage-bot
```

This uses the same chunking/embedding pipeline and tags the chunks under the official project id.

How it works (quick version):
- Reads markdown files in the generated docs folder (default `docs/generated/<custom>`).
- Converts headings/code fences into chunks, splits long prose, generates embeddings, and upserts into Qdrant + SQLite with the provided `--project` id.
- Uses content hashes to skip unchanged files and marks missing files from the folder as orphaned.

Prereqs:
- `OPENAI_API_KEY` plus running Qdrant/SQLite (same as the normal scraper).

### Generate + Index docs-agent-v3 Output

If you use `docs-agent-v3` to generate repo docs, this wrapper will generate docs, export a `RELATIONSHIPS.md` file from the handoff JSON, and index everything:

```bash
npm run generate-and-index-docs -- --project polymarket --repo /path/to/repo
```

Split the steps when needed:

```bash
npm run generate-and-index-docs -- --project polymarket --repo /path/to/repo --generate-only
npm run generate-and-index-docs -- --project polymarket --output docs/generated/<custom> --index-only
```

Note: `docs-agent-v3` expects a local repo path (it does not clone). Clone the repo first if needed.
More details in `DOC_INDEXING.md`.
Note: the wrapper uses `ts-node` registration via `--import` to avoid Node 25 loader warnings.

### Discover Third-Party Sources

Automatically discover third-party GitHub repos, tutorials, and blog posts for a project using an agentic search loop:

```bash
npm run collect-sources-for-project -- -p "o1js zkApp examples" -n 20
npm run collect-sources-for-project -- -p "Solana Anchor tutorials" -n 30 -r 70
npm run collect-sources-for-project -- -p "Cosmos SDK modules" -x "docs.cosmos.network"
```

**How it works:**
1. **Seed queries**: Generates initial search queries from your prompt (e.g., "X examples", "X tutorial", "X github")
2. **Tavily search**: Searches the web for relevant sources
3. **LLM evaluation**: Each result is scored by an LLM (XAI Grok) for relevance (0-100), prioritizing third-party code examples over official docs
4. **Diversity tracking**: Limits sources per project/domain to ensure variety
5. **Agentic iteration**: If not enough sources found, the LLM generates new search queries and the loop continues (up to 10 iterations)
6. **Output**: Writes discovered sources to `config/sources/discovered/` as JSON files ready for the scraper

**CLI Options:**

| Flag | Short | Description |
|------|-------|-------------|
| `--prompt <text>` | `-p` | Search prompt describing what to find (required) |
| `--count <number>` | `-n` | Target number of sources (default: 20) |
| `--min-relevance <n>` | `-r` | Minimum relevance score 0-100 (default: 60) |
| `--exclude-domains <d>` | `-x` | Comma-separated domains to exclude (e.g., `docs.x.com,x.com`) |
| `--model <name>` | `-m` | LLM model for evaluation (default: grok-4-1-fast-non-reasoning-latest) |
| `--output <dir>` | `-o` | Output directory (default: ./config/sources) |
| `--verbose` | `-v` | Show detailed evaluation output |

**Environment variables required:**
- `TAVILY_API_KEY` - Tavily API key for web search
- `XAI_API_KEY` - XAI API key for LLM evaluation

**Output files:**
- `config/sources/discovered/<project>-<date>.json` - Source entries ready for scraping
- `config/sources/discovered/<project>-<date>-report.md` - Human-readable report with relevance scores

## Architecture

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│  Document       │────▶│  Database Layer  │◀────│  MCP HTTP       │
│  Scraper        │     │  (Qdrant + FTS)  │     │  Server         │
└─────────────────┘     └──────────────────┘     └─────────────────┘
        │                       │                        │
        ▼                       │                        ▼
  Project Configs               ▼               ┌─────────────────┐
  (config/projects/)      Vector + SQLite      │  Multi-Provider │
                         (project-filtered)    │  LLM Pipeline   │
                                               └────────┬────────┘
                                                        │
                                               ┌────────▼────────┐
                                               │  Agentic Loop   │
                                               │  (Eval + Refine)│
                                               └────────┬────────┘
                                                        │
                                               ┌────────▼────────┐
                                               │  Web Search     │◀── Tavily API
                                               │  (Optional)     │
                                               └────────┬────────┘
                                                        ▼
                                               ┌─────────────────┐
                                               │  Coding Agents  │
                                               │  (Claude, etc.) │
                                               └─────────────────┘
```

**Data Flow:**
1. Scraper indexes documentation into Qdrant (vectors) and SQLite (full-text)
2. Server receives queries via MCP JSON-RPC over HTTP
3. Hybrid search retrieves relevant chunks with project filtering
4. Reranker (configurable LLM) scores and ranks top results
5. Synthesis LLM generates initial answer with citations
6. Agentic loop evaluates and refines answer (optional, configurable)
7. Web search supplements documentation when needed (requires Tavily API)

## Quick Start

### Prerequisites

- Node.js 18+
- Docker (for Qdrant vector database)
- OpenAI API key (for embeddings and LLM synthesis)

### 1. Install Dependencies

```bash
npm install
```

### 2. Configure Environment

```bash
cp .env.example .env
```

Edit `.env` and set your OpenAI API key:
```
OPENAI_API_KEY=sk-your-api-key-here
```

**Important:** Copy `.env` to each package directory (npm workspaces run from package directories):
```bash
cp .env packages/scraper/.env
cp .env packages/server/.env
```

### 3. Start Qdrant (Vector Database)

```bash
docker-compose up -d
```

Verify it's running:
```bash
curl http://localhost:6333/health
```

### 4. Build the Project

```bash
npm run build
```

### 5. Index Documentation

List available projects:
```bash
npm run scraper -- --list
```

Scrape a specific project:
```bash
# Index Mina Protocol docs (legacy mode)
npm run scraper -- -p mina

# Index with intelligent GitHub scraping (recommended)
npm run scraper -- -p mina --use-registry

# Dry run to preview what would be indexed (no LLM costs)
npm run scraper -- -p mina --use-registry --dry-run

# GitHub sources only (skip documentation crawl)
npm run scraper -- -p mina --use-registry --github-only
```

**Scraper Options:**
| Flag | Short | Description |
|------|-------|-------------|
| `--project <id>` | `-p` | Project to scrape (required) |
| `--list` | `-l` | List available projects |
| `--use-registry` | `-r` | Use source registry for intelligent GitHub scraping |
| `--dry-run` | `-d` | Preview what would be indexed (skip LLM calls) |
| `--github-only` | `-g` | Only scrape GitHub sources, skip documentation |
| `--help` | `-h` | Show help |

This will:
- Crawl the project's documentation site
- Parse and chunk the content
- (With `--use-registry`) Run intelligent quality filtering on GitHub sources
- Generate embeddings via OpenAI
- Store in Qdrant (vector) and SQLite (full-text) with project tags

**Note**: First run takes 5-10 minutes per project and costs ~$0.10-0.20 in OpenAI API calls.

### 6. Start the MCP Server

```bash
npm run server
```

The server runs at `http://localhost:3000` by default.

### 7. Test the Server

```bash
# Health check
curl http://localhost:3000/health

# List available tools
curl -X POST http://localhost:3000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"tools/list","id":1}'

# List available projects
curl -X POST http://localhost:3000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"tools/call","params":{"name":"crypto_list_projects","arguments":{}},"id":2}'

# Ask a question (LLM-synthesized answer)
curl -X POST http://localhost:3000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"tools/call","params":{"name":"crypto_ask_docs","arguments":{"question":"How do I create a smart contract?","project":"mina"}},"id":3}'
```

## Available MCP Tools

The server exposes 5 LLM-powered tools via the MCP protocol. All tools (except `crypto_list_projects`) require a `project` parameter.

| Tool | Description | Example Arguments |
|------|-------------|-------------------|
| `crypto_list_projects` | List available documentation projects | `{}` |
| `crypto_ask_docs` | Ask a question, get a synthesized answer with citations | `{"question": "How do state channels work?", "project": "mina"}` |
| `crypto_get_working_example` | Get complete, runnable code for a task | `{"task": "deploy a smart contract", "project": "solana"}` |
| `crypto_explain_error` | Debug an error with root cause and fixes | `{"error": "proof verification failed", "project": "mina", "context": "during zkApp deployment"}` |
| `crypto_search_docs` | Search raw documentation chunks (no LLM synthesis) | `{"query": "IBC protocol", "project": "cosmos", "limit": 5}` |

### Tool Details

#### `crypto_ask_docs`
Returns a comprehensive answer synthesized by GPT-4o, including:
- Direct answer to the question
- Relevant code snippets with imports
- Step-by-step instructions when applicable
- Source URLs for further reading

#### `crypto_get_working_example`
Returns production-ready code including:
- All necessary imports
- Complete implementation
- Setup/configuration instructions
- Usage example

#### `crypto_explain_error`
Returns debugging assistance including:
- What the error means
- Common causes
- Step-by-step fix instructions
- Prevention tips

#### `crypto_search_docs`
Returns raw documentation chunks for cases where you need:
- Direct access to source material
- Multiple perspectives on a topic
- Content type filtering (prose, code, api_reference)

## Adding New Projects

### Quick Method: Use the Onboarding Wizard (Recommended)

If you're using Claude Code, run the interactive onboarding command:

```
/onboard_project
```

This wizard will:
1. **Gather project information** - name, ID, documentation URL
2. **Configure scraping settings** - Cloudflare protection, URL exclusions, page limits
3. **Set up GitHub sources** - repository, trust level, file patterns (optional)
4. **Create all config files** - project config, source configs, project-sources mapping
5. **Validate and verify** - checks JSON syntax and confirms project appears in scraper

Example session:
```
User: /onboard_project

## Intelligent GitHub Scraping

The intelligent scraper uses a multi-stage quality pipeline to ensure only useful examples are indexed, preventing database pollution with benchmarks, internal utilities, and low-value code.

### Source Registry

Instead of a single GitHub repo per project, the source registry (`config/sources/`) allows multiple repos with different trust levels and scrape strategies.

**Directory Structure:**
```
config/sources/
├── mina-o1js-official.json      # Mina: Official o1js SDK
├── mina-zkapp-cli.json          # Mina: Official zkApp CLI
├── solana-spl-official.json     # Solana: Official SPL
├── cosmos-sdk-official.json     # Cosmos: Official SDK
├── secret-network.json          # Secret: Core blockchain
├── secret-js.json               # Secret: JavaScript SDK
├── secret-toolkit.json          # Secret: Rust toolkit
├── beam-core.json               # Beam: Core blockchain
├── beam-web-wallet.json         # Beam: Web wallet
├── beam-shader-sdk.json         # Beam: Shader SDK
├── pirate-core.json             # Pirate Chain: Core
├── pirate-lightwalletd.json     # Pirate Chain: Light wallet
└── project-sources.json         # Maps projects to sources
```

**Example Source Entry** (`config/sources/mina-o1js-official.json`):
```json
{
  "id": "mina-o1js-official",
  "type": "github",
  "repoType": "sdk",
  "trustLevel": "official",
  "repo": "o1-labs/o1js",
  "branch": "main",
  "scrapeStrategy": {
    "apiPaths": ["src/lib/provable/*.ts", "src/lib/mina/*.ts"],
    "exampleDirs": ["src/examples"],
    "exclude": ["**/*.test.ts", "**/benchmarks/**", "**/node_modules/**"],
    "extensions": [".ts", ".tsx"]
  },
  "qualityThresholds": {
    "minDocumentationScore": 20,
    "minLLMRelevanceScore": 40,
    "requireReadme": false
  },
  "versionPackages": ["o1js"],
  "description": "Official o1js SDK"
}
```

### Trust Levels

Sources are assigned trust levels that affect search ranking:

| Trust Level | Description | Search Weight |
|-------------|-------------|---------------|
| `official` | Official project repos maintained by core team | 1.0 |
| `verified-community` | High-quality community repos (>100 stars, active) | 0.85 |
| `community` | General community contributions | 0.7 |

### Repo Types

Different repo types use different scrape strategies:

| Repo Type | Strategy |
|-----------|----------|
| `sdk` | Only scrape `apiPaths` (API definitions) and `exampleDirs` |
| `example-repo` | Scrape broadly from `exampleDirs`, allow shallow root files |
| `tutorial-repo` | Similar to example-repo, prioritizes documentation |
| `ecosystem-lib` | Community libraries, scrape examples only |

### Quality Pipeline

The intelligent scraper runs a 4-stage quality pipeline:

```
1. Directory Filtering    → Skip files not in allowed paths
2. Quick Heuristics       → Skip tests, configs, benchmarks, generated files
3. Documentation Score    → Assess README, JSDoc, comment density (0-100)
4. LLM Relevance Score    → GPT-4o-mini evaluates "Is this a useful example?" (0-100)
```

Files must pass all stages to be indexed. The LLM prompt is strict:
- **80-100**: Complete, well-documented example teaching a clear concept
- **60-79**: Useful but may lack context
- **40-59**: Partial utility, reference only
- **20-39**: Internal code, not educational
- **0-19**: Boilerplate, benchmarks, irrelevant

### Indexed Metadata

Each chunk from intelligent scraping includes rich metadata:

| Field | Description |
|-------|-------------|
| `trustLevel` | Source trust level (official/verified-community/community) |
| `sourceId` | Reference to source registry entry |
| `qualityScore` | LLM relevance score (0-100) |
| `exampleDescription` | What this code demonstrates |
| `prerequisites` | Inferred prerequisites |
| `versionHint` | Inferred version compatibility |
| `readmeContext` | Relevant README excerpt |
| `repoStats` | Stars, forks, last commit |

### Adding Community Sources

1. Create a source file in `config/sources/`:
```json
{
  "id": "mina-community-examples",
  "type": "github",
  "repoType": "example-repo",
  "trustLevel": "community",
  "repo": "community-member/zkapp-examples",
  "branch": "main",
  "scrapeStrategy": {
    "exampleDirs": ["examples", "demos"],
    "exclude": ["**/*.test.ts", "**/node_modules/**"]
  },
  "qualityThresholds": {
    "minDocumentationScore": 30,
    "minLLMRelevanceScore": 50,
    "requireReadme": true
  },
  "description": "Community zkApp examples"
}
```

2. Add to project mapping in `config/sources/project-sources.json`:
```json
[
  {
    "projectId": "mina",
    "sources": ["mina-o1js-official", "mina-zkapp-cli", "mina-community-examples"]
  }
]
```

3. Run the intelligent scraper:
```bash
npm run scraper -- -p mina --use-registry
```

## Integration with AI Coding Agents

### Claude Desktop

Add to `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "crypto-docs": {
      "url": "http://localhost:3000/mcp",
      "transport": "http"
    }
  }
}
```

### Cursor

Configure MCP in Cursor settings with the HTTP endpoint:
```
http://localhost:3000/mcp
```

### Claude Code CLI

The server works with any MCP-compatible client via the HTTP endpoint.

## LLM Provider Configuration

The server supports multiple LLM providers with automatic detection and per-purpose configuration.

### Supported Providers

| Provider | API Key Variable | Models |
|----------|-----------------|--------|
| OpenAI | `OPENAI_API_KEY` | gpt-4o, gpt-4o-mini, gpt-4.1, gpt-4.1-mini, o3, o4-mini |
| Anthropic | `ANTHROPIC_API_KEY` | claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001, claude-opus-4-5-20251101 |
| XAI | `XAI_API_KEY` | grok-3, grok-3-mini, grok-4, grok-4-fast-reasoning |
| AWS Bedrock | AWS credentials | moonshot.kimi-k2-thinking, anthropic.claude-3-5-sonnet, amazon.nova-pro-v1:0 |

### Auto-Detection

The server automatically selects the first available provider based on configured API keys in this order: OpenAI → Anthropic → XAI → Bedrock.

### Per-Purpose Configuration

Different LLM tasks can use different providers and models for optimal cost/quality balance:

| Purpose | Env Variable (Provider) | Env Variable (Model) | Recommended |
|---------|------------------------|---------------------|-------------|
| **Synthesis** | `LLM_SYNTHESIS_PROVIDER` | `LLM_SYNTHESIS_MODEL` | Best model (accuracy critical) |
| **Evaluation** | `LLM_EVALUATION_PROVIDER` | `LLM_EVALUATION_MODEL` | Fast model (quality assessment) |
| **Refinement** | `LLM_REFINEMENT_PROVIDER` | `LLM_REFINEMENT_MODEL` | Good model (answer improvement) |
| **Analyzer** | `LLM_ANALYZER_PROVIDER` | `LLM_ANALYZER_MODEL` | Fast model (web result filtering) |

### Example Configuration

```bash
# Use Claude for synthesis (highest quality), GPT-4o-mini for evaluation (fast/cheap)
LLM_SYNTHESIS_PROVIDER=anthropic
LLM_SYNTHESIS_MODEL=claude-sonnet-4-5-20250929
LLM_EVALUATION_PROVIDER=openai
LLM_EVALUATION_MODEL=gpt-4o-mini
```

### AWS Bedrock Setup

For AWS Bedrock, use the standard AWS credential chain:

```bash
# Option 1: Environment variables
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1

# Option 2: AWS CLI profile (recommended)
aws configure --profile crypto-docs
AWS_PROFILE=crypto-docs

# Option 3: IAM role (when running on AWS)
# Automatic via instance metadata
```

## Project Structure

```
crypto-docs-mcp/
├── config/
│   ├── projects/              # Project configuration files (7 projects)
│   │   ├── mina.json
│   │   ├── solana.json
│   │   ├── cosmos.json
│   │   ├── secret.json
│   │   ├── beam.json
│   │   ├── pirate-chain.json
│   │   └── polymarket.json
│   └── sources/               # Source registry (29 sources)
│       ├── mina-o1js-official.json
│       ├── mina-zkapp-cli.json
│       ├── solana-spl-official.json
│       ├── cosmos-sdk-official.json
│       ├── secret-*.json          # 7 Secret Network sources
│       ├── beam-*.json            # 6 Beam sources
│       ├── pirate-*.json          # 7 Pirate Chain sources
│       ├── polymarket-*.json      # 2 Polymarket sources
│       └── project-sources.json   # Maps projects to their sources
├── packages/
│   ├── shared/                # Types, DB clients, search logic, LLM integration
│   │   ├── src/
│   │   │   ├── config/
│   │   │   │   ├── project-config.ts
│   │   │   │   ├── load-config.ts
│   │   │   │   ├── source-registry.ts   # Source registry schemas
│   │   │   │   └── load-sources.ts      # Source registry loader
│   │   │   ├── db/            # Qdrant & SQLite clients
│   │   │   ├── types.ts       # Shared types (with trust metadata)
│   │   │   ├── search.ts      # Hybrid search with RRF
│   │   │   ├── reranker.ts    # LLM-based reranking
│   │   │   ├── llm.ts         # Multi-provider LLM client
│   │   │   ├── embeddings.ts  # OpenAI embeddings
│   │   │   ├── corrective-rag.ts        # Low-confidence retry logic
│   │   │   ├── confidence.ts            # Confidence scoring
│   │   │   ├── query-analyzer.ts        # Query intent classification
│   │   │   ├── web-search.ts            # Tavily web search client
│   │   │   ├── web-result-analyzer.ts   # LLM web result filtering
│   │   │   ├── evaluation-orchestrator.ts  # Agentic evaluation loop
│   │   │   ├── evaluator.ts             # Answer quality assessment
│   │   │   ├── evaluation-types.ts      # Evaluation type definitions
│   │   │   ├── answer-refiner.ts        # Answer improvement via LLM
│   │   │   └── adjacent-chunks.ts       # Adjacent chunk expansion
│   │   └── package.json
│   ├── scraper/               # Documentation crawler
│   │   ├── src/
│   │   │   ├── crawler.ts
│   │   │   ├── parser.ts
│   │   │   ├── chunker.ts
│   │   │   ├── github-source.ts           # Legacy GitHub scraper
│   │   │   ├── intelligent-github-scraper.ts  # Quality-filtered scraper
│   │   │   ├── quality-assessor.ts        # LLM relevance scoring
│   │   │   ├── readme-extractor.ts        # README context extraction
│   │   │   └── index.ts
│   │   └── package.json
│   ├── server/                # MCP HTTP server
│   │   ├── src/
│   │   │   ├── tools/         # MCP tool implementations
│   │   │   │   ├── ask-docs.ts
│   │   │   │   ├── working-example.ts
│   │   │   │   ├── explain-error.ts
│   │   │   │   ├── search-docs.ts
│   │   │   │   └── list-projects.ts
│   │   │   ├── prompts/       # LLM system prompts
│   │   │   ├── resources/
│   │   │   ├── transport.ts
│   │   │   └── index.ts
│   │   └── package.json
│   └── evaluator/             # Test harness for quality assurance
│       ├── src/
│       │   ├── index.ts       # CLI entry point
│       │   ├── harness.ts     # Test runner
│       │   ├── metrics.ts     # Pass/fail metrics
│       │   ├── reporter.ts    # Report generation
│       │   └── validators/    # Validation logic
│       │       └── llm-judge.ts  # GPT-4o-mini scoring
│       ├── datasets/          # YAML test cases
│       │   ├── mina/
│       │   ├── solana/
│       │   └── cosmos/
│       ├── reports/           # Evaluation results
│       └── package.json
├── scripts/                   # Demo and test scripts
│   ├── demo.ts
│   ├── demo.sh
│   └── test-integration.ts
├── data/                      # SQLite database storage
├── docker-compose.yml         # Qdrant setup
└── package.json               # Monorepo root
```

## Development

```bash
# Build all packages
npm run build

# Clean build artifacts
npm run clean

# List available projects
npm run scraper -- --list

# Index a project (legacy mode)
npm run scraper -- -p mina

# Index with intelligent scraping (recommended)
npm run scraper -- -p mina --use-registry

# Dry run (preview without LLM costs)
npm run scraper -- -p mina --use-registry --dry-run

# GitHub only (skip docs crawl)
npm run scraper -- -p mina --use-registry --github-only

# Start server (production)
npm run server

# Start server in dev mode (with watch)
npm run dev:server
```

### Demo Scripts

Test the server with interactive demos:

```bash
# TypeScript demo (compiles first)
npm run demo

# Quick demo (pre-compiled)
npm run demo:quick

# Bash demo (shell script)
npm run demo:bash

# Or run directly with a specific project
./scripts/demo.sh solana
```

### Integration Tests

Run the integration test suite:

```bash
# Requires server to be running
npm run test:integration
```

Tests cover: health check, MCP initialization, all 5 tools, and resources.

## Evaluation Suite

The project includes a comprehensive evaluation system for testing answer quality using LLM-based validation.

### Running Evaluations

```bash
# Run all evaluations
npm run eval

# Project-specific evaluations
npm run eval:mina
npm run eval:solana
npm run eval:cosmos

# Test fallback behavior
npm run eval:fallback

# Verbose output with details
npm run eval:verbose

# Preview tests without running
npm run eval:dry-run
```

### Evaluator CLI Options

| Flag | Description |
|------|-------------|
| `--project <id>` | Filter by project (mina/solana/cosmos) |
| `--tag <tag>` | Filter by tag (e.g., zkapp, concept) |
| `--difficulty <level>` | Filter by difficulty (basic/intermediate/advanced) |
| `--output <path>` | Save JSON report to file |
| `--verbose` | Show detailed output |
| `--fail-fast` | Stop on first failure |
| `--dry-run` | List tests without running |
| `--ci` | Exit with code 1 on failures (for CI/CD) |

### Test Datasets

Test cases are defined in YAML files under `packages/evaluator/datasets/`:
- **Mina**: primitives, zkapp, proofs, data-structures, tokens-actions
- **Solana**: accounts, tokens, transactions
- **Cosmos**: modules, ibc, transactions, accounts, staking
- **Cross-project**: multi-hop queries, fallback behavior

### Validation Types

- **Basic validators**: Min length, citation presence, keyword checks
- **Ground truth**: Expected facts and forbidden claims
- **Code requirements**: Import completeness, setup instructions
- **LLM Judge**: GPT-4o-mini scores relevance, accuracy, completeness (0-100)

## RAG Inspector

Interactive CLI for testing and evaluating the corrective RAG system. Simulates coding agent queries and shows detailed metrics about how the RAG pipeline processes them.

### Running the Inspector

```bash
# Start the MCP server first (in another terminal)
npm run server

# Run the RAG Inspector (compiles TypeScript on first run)
npm run rag-inspector

# Or with a specific project (uses pre-compiled JS)
npm run rag:mina
npm run rag:solana
npm run rag:cosmos
npm run rag:secret
npm run rag:beam
npm run rag:pirate-chain
npm run rag:polymarket
```

See `docs/RAG_TESTING.md` for prerequisites and per-project commands for every onboarded platform.

### Inspector Commands

| Command | Description |
|---------|-------------|
| `ask <question>` | Ask a question and see detailed RAG analysis |
| `a <question>` | Shorthand for ask |
| `compare` | Compare two queries side by side (interactive) |
| `stress <topic>` | Run stress test with 8 query variations |
| `retry` | Re-run the last query |
| `history` | Show query history with metrics summary |
| `detail <n>` | Show full details for history item n |
| `project <id>` | Switch project (mina, solana, cosmos, secret, beam, pirate-chain, polymarket) |
| `export [file]` | Export session history to JSON |
| `clear` | Clear screen |
| `help` | Show help |
| `quit` | Exit |

### What the Inspector Shows

For each query, you'll see:

- **Confidence Meter**: Visual 0-100% bar (green ≥70%, yellow ≥50%, red <50%)
- **RAG Metrics**: Retrieval quality, sources used, query type, processing time
- **Corrective RAG Indicator**: Shows when retries were triggered and what alternative queries were tried
- **Search Guidance**: When docs are insufficient, shows suggested web searches
- **Suggestions**: Follow-up actions and related queries
- **Answer Preview**: First 500 characters of the response

### Special Modes

**Compare Mode** (`compare`):
Enter two queries and see a side-by-side comparison of confidence, quality, and whether search guidance was triggered.

**Stress Test** (`stress <topic>`):
Runs 8 variations of a query (e.g., "What is X?", "How do I use X?", "X example code") and shows aggregate statistics including average confidence and quality distribution.

**Export** (`export session.json`):
Saves full session history with raw responses for offline analysis.

## Agentic Evaluation Loop

The server includes an optional agentic evaluation system that iteratively improves answers using LLM self-assessment and web search.

### How It Works

1. **Initial Answer**: Generate answer from documentation using synthesis LLM
2. **Self-Assessment**: Evaluation LLM scores the answer for completeness, accuracy, and relevance
3. **Decision Point**: If score is below threshold:
   - Query additional documentation with refined search terms
   - Search the web via Tavily API for supplementary information
   - Or return if answer is already sufficient
4. **Refinement**: Refinement LLM improves the answer with new context
5. **Iterate**: Repeat until confidence threshold met or max iterations reached

### Configuration

```bash
# Enable/disable the agentic loop (default: true)
AGENTIC_EVALUATION_ENABLED=true

# Maximum evaluation iterations (default: 3)
AGENTIC_MAX_ITERATIONS=3

# Confidence threshold to skip evaluation (0-100, default: 85)
AGENTIC_AUTO_RETURN_THRESHOLD=85

# Max web searches per query (default: 2)
AGENTIC_MAX_WEB_SEARCHES=2

# Max additional doc queries per iteration (default: 2)
AGENTIC_MAX_DOC_QUERIES=2
```

### Web Search Integration

When documentation is insufficient, the system can search the web via Tavily API:

```bash
# Required for web search
TAVILY_API_KEY=tvly-...

# Search depth: basic or advanced (default: basic)
TAVILY_SEARCH_DEPTH=basic

# Maximum results per search (default: 5)
TAVILY_MAX_RESULTS=5
```

The analyzer LLM filters web results for relevance before incorporating them into the answer.

### Disabling Agentic Evaluation

For faster responses (single-pass mode), disable the evaluation loop:

```bash
AGENTIC_EVALUATION_ENABLED=false
```

This returns the initial synthesis without iterative improvement, reducing latency and API costs.

## Environment Variables

### API Keys (at least one LLM provider required)

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | * | OpenAI API key (also used for embeddings) |
| `ANTHROPIC_API_KEY` | * | Anthropic API key for Claude models |
| `XAI_API_KEY` | * | XAI API key for Grok models |
| `AWS_ACCESS_KEY_ID` | * | AWS credentials for Bedrock (or use AWS credential chain) |
| `AWS_SECRET_ACCESS_KEY` | * | AWS credentials for Bedrock |
| `AWS_REGION` | No | AWS region for Bedrock (default: us-east-1) |
| `GITHUB_TOKEN` | No | GitHub token for higher API rate limits |
| `TAVILY_API_KEY` | No | Tavily API key for web search in agentic loop |

\* At least one LLM provider must be configured.

### LLM Provider Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_PROVIDER` | auto-detect | Default provider (openai/anthropic/xai/bedrock) |
| `LLM_SYNTHESIS_PROVIDER` | (inherit) | Override provider for answer synthesis |
| `LLM_SYNTHESIS_MODEL` | gpt-4o | Model for answer synthesis |
| `LLM_EVALUATION_PROVIDER` | (inherit) | Override provider for quality evaluation |
| `LLM_EVALUATION_MODEL` | gpt-4o-mini | Model for answer quality assessment |
| `LLM_REFINEMENT_PROVIDER` | (inherit) | Override provider for answer refinement |
| `LLM_REFINEMENT_MODEL` | gpt-4o | Model for answer improvement |
| `LLM_ANALYZER_PROVIDER` | (inherit) | Override provider for web result analysis |
| `LLM_ANALYZER_MODEL` | gpt-4o-mini | Model for filtering web results |
| `LLM_MAX_TOKENS` | 4000 | Maximum tokens for LLM responses |
| `LLM_TEMPERATURE` | 0.3 | LLM temperature (lower = more focused) |

### Agentic Evaluation

| Variable | Default | Description |
|----------|---------|-------------|
| `AGENTIC_EVALUATION_ENABLED` | true | Enable/disable agentic evaluation loop |
| `AGENTIC_MAX_ITERATIONS` | 3 | Maximum evaluation iterations |
| `AGENTIC_AUTO_RETURN_THRESHOLD` | 85 | Confidence threshold (0-100) to skip evaluation |
| `AGENTIC_MAX_WEB_SEARCHES` | 2 | Maximum web searches per query |
| `AGENTIC_MAX_DOC_QUERIES` | 2 | Maximum additional doc queries per iteration |

### Web Search (Tavily)

| Variable | Default | Description |
|----------|---------|-------------|
| `TAVILY_SEARCH_DEPTH` | basic | Search depth (basic/advanced) |
| `TAVILY_MAX_RESULTS` | 5 | Maximum results per search |

### Database Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `QDRANT_URL` | http://localhost:6333 | Qdrant vector database URL |
| `QDRANT_COLLECTION` | crypto_docs | Qdrant collection name |
| `SQLITE_PATH` | ./data/crypto_docs.db | SQLite database path |

### Server Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `MCP_PORT` | 3000 | Server port |
| `MCP_HOST` | localhost | Server host |

## How It Works

### Indexing Pipeline
1. **Project Config** defines documentation URL, GitHub repo, and crawler settings
2. **Source Registry** (optional) defines multiple GitHub sources with trust levels
3. **Scraper** crawls the documentation site for a specific project
4. **Parser** converts HTML to structured chunks (prose, code, API reference)
5. **Intelligent GitHub Scraper** applies 4-stage quality filtering on code
6. **Chunker** splits large content with semantic overlap
7. **Embeddings** are generated via OpenAI text-embedding-3-small
8. **Qdrant** stores vectors for semantic search (with project tags)
9. **SQLite FTS5** provides fast full-text search (with project filtering)

### Query Pipeline
10. **Hybrid Search** combines vector and full-text using Reciprocal Rank Fusion
11. **Reranker** (configurable LLM) scores top candidates for relevance
12. **Corrective RAG** retries with alternative queries if confidence is low
13. **Synthesis LLM** generates initial answer with citations

### Agentic Evaluation Loop (optional)
14. **Evaluation LLM** assesses answer quality (completeness, accuracy, relevance)
15. **Web Search** (Tavily) fetches supplementary information if needed
16. **Analyzer LLM** filters web results for relevance
17. **Refinement LLM** improves answer with additional context
18. **Iterate** until confidence threshold met or max iterations reached

### Serving
19. **MCP Server** exposes tools via JSON-RPC over HTTP

## API Reference

### HTTP Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check with version info |
| `/mcp` | POST | MCP JSON-RPC 2.0 endpoint |
| `/mcp/events` | GET | Server-Sent Events (SSE) stream |

### Health Response

```json
{
  "status": "ok",
  "server": "crypto-docs-mcp",
  "version": "0.1.0",
  "features": ["llm-synthesis", "reranking"],
  "endpoints": {
    "mcp": "/mcp",
    "health": "/health"
  }
}
```

### MCP Methods

| Method | Description |
|--------|-------------|
| `initialize` | Initialize MCP connection |
| `tools/list` | List available tools |
| `tools/call` | Execute a tool |
| `resources/list` | List available resources |
| `resources/read` | Read a resource by URI |
| `ping` | Connection keepalive |

## Troubleshooting

### Qdrant Connection Failed
```bash
# Make sure Docker is running
docker-compose up -d

# Check Qdrant health
curl http://localhost:6333/health
```

### Empty Search Results
Run the scraper first to index documentation:
```bash
npm run scraper -- -p mina --use-registry
```

### OpenAI API Errors
Verify your API key is set correctly in `.env`:
```bash
echo $OPENAI_API_KEY
```

### "OPENAI_API_KEY environment variable is required"
The `.env` file must be present in the package directory being run. Copy it to both:
```bash
cp .env packages/scraper/.env
cp .env packages/server/.env
```

### "No projects configured"
Make sure project JSON files exist in `config/projects/` directory.

### LLM Synthesis Too Slow
- Disable agentic evaluation: `AGENTIC_EVALUATION_ENABLED=false` (single-pass mode)
- Reduce `LLM_MAX_TOKENS` for shorter responses
- Use faster models for evaluation/refinement (e.g., `gpt-4o-mini` or `claude-haiku`)
- Use `crypto_search_docs` tool for raw results without LLM processing

## License

MIT

## Resources

- [Model Context Protocol](https://modelcontextprotocol.io)
- [Mina Protocol Documentation](https://docs.minaprotocol.com)
- [Solana Documentation](https://solana.com/docs)
- [Cosmos SDK Documentation](https://docs.cosmos.network)
- [Secret Network Documentation](https://docs.scrt.network)
- [Beam Documentation](https://github.com/BeamMW/beam/wiki)
- [Pirate Chain Documentation](https://docs.piratechain.com)
- [Polymarket Documentation](https://docs.polymarket.com)
