# Crypto Documentation MCP Server

A Model Context Protocol (MCP) server that provides blockchain developer documentation to AI coding agents like Claude Code and Cursor. Uses LLM-powered answer synthesis to deliver comprehensive responses with code examples, imports, and source citations. Supports 6 crypto projects: Mina Protocol, Solana, Cosmos SDK, Secret Network, Beam, and Pirate Chain.

## Features

- **LLM-Synthesized Answers**: Get comprehensive answers generated by GPT-4o, not just raw documentation chunks
- **Multi-Project Support**: Query documentation from 6 projects: Mina, Solana, Cosmos, Secret Network, Beam, and Pirate Chain
- **Hybrid Search**: Combines vector similarity (Qdrant) and full-text search (SQLite FTS5) with Reciprocal Rank Fusion
- **Smart Reranking**: Results are reranked using gpt-4o-mini for higher relevance
- **Corrective RAG**: Automatically retries with alternative queries when initial results have low confidence
- **Working Code Examples**: Get complete, runnable code with imports and setup instructions
- **Error Debugging**: Explain errors with root cause analysis and fixes
- **Source Citations**: All answers include links to original documentation sources
- **Intelligent GitHub Scraping**: Quality-filtered indexing of GitHub examples with LLM relevance scoring
- **Trust Levels**: Tiered source trust (official > verified-community > community) affects search ranking
- **Source Registry**: Modular configuration for multiple GitHub repos per project
- **Evaluation Suite**: Comprehensive test harness with LLM-based validation for quality assurance

## Supported Projects

| Project | ID | Documentation | Source Code |
|---------|-----|--------------|-------------|
| Mina Protocol | `mina` | docs.minaprotocol.com | o1-labs/o1js, o1-labs/zkapp-cli |
| Solana | `solana` | solana.com/docs | solana-labs/solana-program-library |
| Cosmos SDK | `cosmos` | docs.cosmos.network | cosmos/cosmos-sdk |
| Secret Network | `secret` | docs.scrt.network | scrtlabs/SecretNetwork, scrtlabs/secret.js, scrtlabs/secret-toolkit |
| Beam | `beam` | github.com/BeamMW/beam/wiki | BeamMW/beam, BeamMW/web-wallet, BeamMW/shader-sdk |
| Pirate Chain | `pirate-chain` | docs.piratechain.com | PirateNetwork/pirate, PirateNetwork/lightwalletd |

### Quick Index Commands

```bash
# Index all projects (recommended with --use-registry for GitHub quality filtering)
npm run scraper -- -p mina --use-registry
npm run scraper -- -p solana --use-registry
npm run scraper -- -p cosmos --use-registry
npm run scraper -- -p secret --use-registry
npm run scraper -- -p beam --use-registry
npm run scraper -- -p pirate-chain --use-registry
```

Add more projects by creating a configuration file in `config/projects/`.

### Index Generated Repository Docs

Index locally generated documentation (e.g. from `/generate_repo_docs`) under an existing project:

```bash
npm run index:generated-docs -- --project polymarket --custom Polymarket-Kalshi-Arbitrage-bot
# Or point directly at a folder:
npm run index:generated-docs -- --project polymarket --dir docs/generated/Polymarket-Kalshi-Arbitrage-bot
```

For the existing generated docs in this repo, run:

```bash
npm run index:generated-docs -- --project polymarket --dir docs/generated/Polymarket-Kalshi-Arbitrage-bot
```

This uses the same chunking/embedding pipeline and tags the chunks under the official project id.

How it works (quick version):
- Reads markdown files in the generated docs folder (default `docs/generated/<custom>`).
- Converts headings/code fences into chunks, splits long prose, generates embeddings, and upserts into Qdrant + SQLite with the provided `--project` id.
- Uses content hashes to skip unchanged files and marks missing files from the folder as orphaned.

Prereqs:
- `OPENAI_API_KEY` plus running Qdrant/SQLite (same as the normal scraper).

## Architecture

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│  Document       │────▶│  Database Layer  │◀────│  MCP HTTP       │
│  Scraper        │     │  (Qdrant + FTS)  │     │  Server         │
└─────────────────┘     └──────────────────┘     └─────────────────┘
        │                       │                        │
        ▼                       │                        ▼
  Project Configs               ▼               ┌─────────────────┐
  (config/projects/)      Vector + SQLite      │  LLM Pipeline   │
                         (project-filtered)    │  (GPT-4o)       │
                                               └────────┬────────┘
                                                        ▼
                                               ┌─────────────────┐
                                               │  Coding Agents  │
                                               │  (Claude, etc.) │
                                               └─────────────────┘
```

**Data Flow:**
1. Scraper indexes documentation into Qdrant (vectors) and SQLite (full-text)
2. Server receives queries via MCP JSON-RPC over HTTP
3. Hybrid search retrieves relevant chunks with project filtering
4. Reranker (gpt-4o-mini) scores and ranks top results
5. LLM (GPT-4o) synthesizes comprehensive answer with citations

## Quick Start

### Prerequisites

- Node.js 18+
- Docker (for Qdrant vector database)
- OpenAI API key (for embeddings and LLM synthesis)

### 1. Install Dependencies

```bash
npm install
```

### 2. Configure Environment

```bash
cp .env.example .env
```

Edit `.env` and set your OpenAI API key:
```
OPENAI_API_KEY=sk-your-api-key-here
```

**Important:** Copy `.env` to each package directory (npm workspaces run from package directories):
```bash
cp .env packages/scraper/.env
cp .env packages/server/.env
```

### 3. Start Qdrant (Vector Database)

```bash
docker-compose up -d
```

Verify it's running:
```bash
curl http://localhost:6333/health
```

### 4. Build the Project

```bash
npm run build
```

### 5. Index Documentation

List available projects:
```bash
npm run scraper -- --list
```

Scrape a specific project:
```bash
# Index Mina Protocol docs (legacy mode)
npm run scraper -- -p mina

# Index with intelligent GitHub scraping (recommended)
npm run scraper -- -p mina --use-registry

# Dry run to preview what would be indexed (no LLM costs)
npm run scraper -- -p mina --use-registry --dry-run

# GitHub sources only (skip documentation crawl)
npm run scraper -- -p mina --use-registry --github-only
```

**Scraper Options:**
| Flag | Short | Description |
|------|-------|-------------|
| `--project <id>` | `-p` | Project to scrape (required) |
| `--list` | `-l` | List available projects |
| `--use-registry` | `-r` | Use source registry for intelligent GitHub scraping |
| `--dry-run` | `-d` | Preview what would be indexed (skip LLM calls) |
| `--github-only` | `-g` | Only scrape GitHub sources, skip documentation |
| `--help` | `-h` | Show help |

This will:
- Crawl the project's documentation site
- Parse and chunk the content
- (With `--use-registry`) Run intelligent quality filtering on GitHub sources
- Generate embeddings via OpenAI
- Store in Qdrant (vector) and SQLite (full-text) with project tags

**Note**: First run takes 5-10 minutes per project and costs ~$0.10-0.20 in OpenAI API calls.

### 6. Start the MCP Server

```bash
npm run server
```

The server runs at `http://localhost:3000` by default.

### 7. Test the Server

```bash
# Health check
curl http://localhost:3000/health

# List available tools
curl -X POST http://localhost:3000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"tools/list","id":1}'

# List available projects
curl -X POST http://localhost:3000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"tools/call","params":{"name":"crypto_list_projects","arguments":{}},"id":2}'

# Ask a question (LLM-synthesized answer)
curl -X POST http://localhost:3000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"tools/call","params":{"name":"crypto_ask_docs","arguments":{"question":"How do I create a smart contract?","project":"mina"}},"id":3}'
```

## Available MCP Tools

The server exposes 5 LLM-powered tools via the MCP protocol. All tools (except `crypto_list_projects`) require a `project` parameter.

| Tool | Description | Example Arguments |
|------|-------------|-------------------|
| `crypto_list_projects` | List available documentation projects | `{}` |
| `crypto_ask_docs` | Ask a question, get a synthesized answer with citations | `{"question": "How do state channels work?", "project": "mina"}` |
| `crypto_get_working_example` | Get complete, runnable code for a task | `{"task": "deploy a smart contract", "project": "solana"}` |
| `crypto_explain_error` | Debug an error with root cause and fixes | `{"error": "proof verification failed", "project": "mina", "context": "during zkApp deployment"}` |
| `crypto_search_docs` | Search raw documentation chunks (no LLM synthesis) | `{"query": "IBC protocol", "project": "cosmos", "limit": 5}` |

### Tool Details

#### `crypto_ask_docs`
Returns a comprehensive answer synthesized by GPT-4o, including:
- Direct answer to the question
- Relevant code snippets with imports
- Step-by-step instructions when applicable
- Source URLs for further reading

#### `crypto_get_working_example`
Returns production-ready code including:
- All necessary imports
- Complete implementation
- Setup/configuration instructions
- Usage example

#### `crypto_explain_error`
Returns debugging assistance including:
- What the error means
- Common causes
- Step-by-step fix instructions
- Prevention tips

#### `crypto_search_docs`
Returns raw documentation chunks for cases where you need:
- Direct access to source material
- Multiple perspectives on a topic
- Content type filtering (prose, code, api_reference)

## Adding New Projects

### Quick Method: Use the Onboarding Wizard (Recommended)

If you're using Claude Code, run the interactive onboarding command:

```
/onboard_project
```

This wizard will:
1. **Gather project information** - name, ID, documentation URL
2. **Configure scraping settings** - Cloudflare protection, URL exclusions, page limits
3. **Set up GitHub sources** - repository, trust level, file patterns (optional)
4. **Create all config files** - project config, source configs, project-sources mapping
5. **Validate and verify** - checks JSON syntax and confirms project appears in scraper

Example session:
```
User: /onboard_project

## Intelligent GitHub Scraping

The intelligent scraper uses a multi-stage quality pipeline to ensure only useful examples are indexed, preventing database pollution with benchmarks, internal utilities, and low-value code.

### Source Registry

Instead of a single GitHub repo per project, the source registry (`config/sources/`) allows multiple repos with different trust levels and scrape strategies.

**Directory Structure:**
```
config/sources/
├── mina-o1js-official.json      # Mina: Official o1js SDK
├── mina-zkapp-cli.json          # Mina: Official zkApp CLI
├── solana-spl-official.json     # Solana: Official SPL
├── cosmos-sdk-official.json     # Cosmos: Official SDK
├── secret-network.json          # Secret: Core blockchain
├── secret-js.json               # Secret: JavaScript SDK
├── secret-toolkit.json          # Secret: Rust toolkit
├── beam-core.json               # Beam: Core blockchain
├── beam-web-wallet.json         # Beam: Web wallet
├── beam-shader-sdk.json         # Beam: Shader SDK
├── pirate-core.json             # Pirate Chain: Core
├── pirate-lightwalletd.json     # Pirate Chain: Light wallet
└── project-sources.json         # Maps projects to sources
```

**Example Source Entry** (`config/sources/mina-o1js-official.json`):
```json
{
  "id": "mina-o1js-official",
  "type": "github",
  "repoType": "sdk",
  "trustLevel": "official",
  "repo": "o1-labs/o1js",
  "branch": "main",
  "scrapeStrategy": {
    "apiPaths": ["src/lib/provable/*.ts", "src/lib/mina/*.ts"],
    "exampleDirs": ["src/examples"],
    "exclude": ["**/*.test.ts", "**/benchmarks/**", "**/node_modules/**"],
    "extensions": [".ts", ".tsx"]
  },
  "qualityThresholds": {
    "minDocumentationScore": 20,
    "minLLMRelevanceScore": 40,
    "requireReadme": false
  },
  "versionPackages": ["o1js"],
  "description": "Official o1js SDK"
}
```

### Trust Levels

Sources are assigned trust levels that affect search ranking:

| Trust Level | Description | Search Weight |
|-------------|-------------|---------------|
| `official` | Official project repos maintained by core team | 1.0 |
| `verified-community` | High-quality community repos (>100 stars, active) | 0.85 |
| `community` | General community contributions | 0.7 |

### Repo Types

Different repo types use different scrape strategies:

| Repo Type | Strategy |
|-----------|----------|
| `sdk` | Only scrape `apiPaths` (API definitions) and `exampleDirs` |
| `example-repo` | Scrape broadly from `exampleDirs`, allow shallow root files |
| `tutorial-repo` | Similar to example-repo, prioritizes documentation |
| `ecosystem-lib` | Community libraries, scrape examples only |

### Quality Pipeline

The intelligent scraper runs a 4-stage quality pipeline:

```
1. Directory Filtering    → Skip files not in allowed paths
2. Quick Heuristics       → Skip tests, configs, benchmarks, generated files
3. Documentation Score    → Assess README, JSDoc, comment density (0-100)
4. LLM Relevance Score    → GPT-4o-mini evaluates "Is this a useful example?" (0-100)
```

Files must pass all stages to be indexed. The LLM prompt is strict:
- **80-100**: Complete, well-documented example teaching a clear concept
- **60-79**: Useful but may lack context
- **40-59**: Partial utility, reference only
- **20-39**: Internal code, not educational
- **0-19**: Boilerplate, benchmarks, irrelevant

### Indexed Metadata

Each chunk from intelligent scraping includes rich metadata:

| Field | Description |
|-------|-------------|
| `trustLevel` | Source trust level (official/verified-community/community) |
| `sourceId` | Reference to source registry entry |
| `qualityScore` | LLM relevance score (0-100) |
| `exampleDescription` | What this code demonstrates |
| `prerequisites` | Inferred prerequisites |
| `versionHint` | Inferred version compatibility |
| `readmeContext` | Relevant README excerpt |
| `repoStats` | Stars, forks, last commit |

### Adding Community Sources

1. Create a source file in `config/sources/`:
```json
{
  "id": "mina-community-examples",
  "type": "github",
  "repoType": "example-repo",
  "trustLevel": "community",
  "repo": "community-member/zkapp-examples",
  "branch": "main",
  "scrapeStrategy": {
    "exampleDirs": ["examples", "demos"],
    "exclude": ["**/*.test.ts", "**/node_modules/**"]
  },
  "qualityThresholds": {
    "minDocumentationScore": 30,
    "minLLMRelevanceScore": 50,
    "requireReadme": true
  },
  "description": "Community zkApp examples"
}
```

2. Add to project mapping in `config/sources/project-sources.json`:
```json
[
  {
    "projectId": "mina",
    "sources": ["mina-o1js-official", "mina-zkapp-cli", "mina-community-examples"]
  }
]
```

3. Run the intelligent scraper:
```bash
npm run scraper -- -p mina --use-registry
```

## Integration with AI Coding Agents

### Claude Desktop

Add to `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "crypto-docs": {
      "url": "http://localhost:3000/mcp",
      "transport": "http"
    }
  }
}
```

### Cursor

Configure MCP in Cursor settings with the HTTP endpoint:
```
http://localhost:3000/mcp
```

### Claude Code CLI

The server works with any MCP-compatible client via the HTTP endpoint.

## Project Structure

```
crypto-docs-mcp/
├── config/
│   ├── projects/              # Project configuration files (6 projects)
│   │   ├── mina.json
│   │   ├── solana.json
│   │   ├── cosmos.json
│   │   ├── secret.json
│   │   ├── beam.json
│   │   └── pirate-chain.json
│   └── sources/               # Source registry (13 sources)
│       ├── mina-o1js-official.json
│       ├── mina-zkapp-cli.json
│       ├── solana-spl-official.json
│       ├── cosmos-sdk-official.json
│       ├── secret-network.json
│       ├── secret-js.json
│       ├── secret-toolkit.json
│       ├── beam-core.json
│       ├── beam-web-wallet.json
│       ├── beam-shader-sdk.json
│       ├── pirate-core.json
│       ├── pirate-lightwalletd.json
│       └── project-sources.json
├── packages/
│   ├── shared/                # Types, DB clients, search logic
│   │   ├── src/
│   │   │   ├── config/
│   │   │   │   ├── project-config.ts
│   │   │   │   ├── load-config.ts
│   │   │   │   ├── source-registry.ts   # Source registry schemas
│   │   │   │   └── load-sources.ts      # Source registry loader
│   │   │   ├── db/            # Qdrant & SQLite clients
│   │   │   ├── types.ts       # Shared types (with trust metadata)
│   │   │   ├── search.ts      # Hybrid search with RRF
│   │   │   ├── reranker.ts    # GPT-4o-mini reranking
│   │   │   ├── llm.ts         # GPT-4o answer synthesis
│   │   │   └── embeddings.ts
│   │   └── package.json
│   ├── scraper/               # Documentation crawler
│   │   ├── src/
│   │   │   ├── crawler.ts
│   │   │   ├── parser.ts
│   │   │   ├── chunker.ts
│   │   │   ├── github-source.ts           # Legacy GitHub scraper
│   │   │   ├── intelligent-github-scraper.ts  # Quality-filtered scraper
│   │   │   ├── quality-assessor.ts        # LLM relevance scoring
│   │   │   ├── readme-extractor.ts        # README context extraction
│   │   │   └── index.ts
│   │   └── package.json
│   ├── server/                # MCP HTTP server
│   │   ├── src/
│   │   │   ├── tools/         # MCP tool implementations
│   │   │   │   ├── ask-docs.ts
│   │   │   │   ├── working-example.ts
│   │   │   │   ├── explain-error.ts
│   │   │   │   ├── search-docs.ts
│   │   │   │   └── list-projects.ts
│   │   │   ├── prompts/       # LLM system prompts
│   │   │   ├── resources/
│   │   │   ├── transport.ts
│   │   │   └── index.ts
│   │   └── package.json
│   └── evaluator/             # Test harness for quality assurance
│       ├── src/
│       │   ├── index.ts       # CLI entry point
│       │   ├── harness.ts     # Test runner
│       │   ├── metrics.ts     # Pass/fail metrics
│       │   ├── reporter.ts    # Report generation
│       │   └── validators/    # Validation logic
│       │       └── llm-judge.ts  # GPT-4o-mini scoring
│       ├── datasets/          # YAML test cases
│       │   ├── mina/
│       │   ├── solana/
│       │   └── cosmos/
│       ├── reports/           # Evaluation results
│       └── package.json
├── scripts/                   # Demo and test scripts
│   ├── demo.ts
│   ├── demo.sh
│   └── test-integration.ts
├── data/                      # SQLite database storage
├── docker-compose.yml         # Qdrant setup
└── package.json               # Monorepo root
```

## Development

```bash
# Build all packages
npm run build

# Clean build artifacts
npm run clean

# List available projects
npm run scraper -- --list

# Index a project (legacy mode)
npm run scraper -- -p mina

# Index with intelligent scraping (recommended)
npm run scraper -- -p mina --use-registry

# Dry run (preview without LLM costs)
npm run scraper -- -p mina --use-registry --dry-run

# GitHub only (skip docs crawl)
npm run scraper -- -p mina --use-registry --github-only

# Start server (production)
npm run server

# Start server in dev mode (with watch)
npm run dev:server
```

### Demo Scripts

Test the server with interactive demos:

```bash
# TypeScript demo (compiles first)
npm run demo

# Quick demo (pre-compiled)
npm run demo:quick

# Bash demo (shell script)
npm run demo:bash

# Or run directly with a specific project
./scripts/demo.sh solana
```

### Integration Tests

Run the integration test suite:

```bash
# Requires server to be running
npm run test:integration
```

Tests cover: health check, MCP initialization, all 5 tools, and resources.

## Evaluation Suite

The project includes a comprehensive evaluation system for testing answer quality using LLM-based validation.

### Running Evaluations

```bash
# Run all evaluations
npm run eval

# Project-specific evaluations
npm run eval:mina
npm run eval:solana
npm run eval:cosmos

# Test fallback behavior
npm run eval:fallback

# Verbose output with details
npm run eval:verbose

# Preview tests without running
npm run eval:dry-run
```

### Evaluator CLI Options

| Flag | Description |
|------|-------------|
| `--project <id>` | Filter by project (mina/solana/cosmos) |
| `--tag <tag>` | Filter by tag (e.g., zkapp, concept) |
| `--difficulty <level>` | Filter by difficulty (basic/intermediate/advanced) |
| `--output <path>` | Save JSON report to file |
| `--verbose` | Show detailed output |
| `--fail-fast` | Stop on first failure |
| `--dry-run` | List tests without running |
| `--ci` | Exit with code 1 on failures (for CI/CD) |

### Test Datasets

Test cases are defined in YAML files under `packages/evaluator/datasets/`:
- **Mina**: primitives, zkapp, proofs, data-structures, tokens-actions
- **Solana**: accounts, tokens, transactions
- **Cosmos**: modules, ibc, transactions, accounts, staking
- **Cross-project**: multi-hop queries, fallback behavior

### Validation Types

- **Basic validators**: Min length, citation presence, keyword checks
- **Ground truth**: Expected facts and forbidden claims
- **Code requirements**: Import completeness, setup instructions
- **LLM Judge**: GPT-4o-mini scores relevance, accuracy, completeness (0-100)

## RAG Inspector

Interactive CLI for testing and evaluating the corrective RAG system. Simulates coding agent queries and shows detailed metrics about how the RAG pipeline processes them.

### Running the Inspector

```bash
# Start the MCP server first (in another terminal)
npm run server

# Run the RAG Inspector (compiles TypeScript on first run)
npm run rag-inspector

# Or with a specific project (uses pre-compiled JS)
npm run rag:mina
npm run rag:solana
npm run rag:cosmos
```

### Inspector Commands

| Command | Description |
|---------|-------------|
| `ask <question>` | Ask a question and see detailed RAG analysis |
| `a <question>` | Shorthand for ask |
| `compare` | Compare two queries side by side (interactive) |
| `stress <topic>` | Run stress test with 8 query variations |
| `retry` | Re-run the last query |
| `history` | Show query history with metrics summary |
| `detail <n>` | Show full details for history item n |
| `project <id>` | Switch project (mina/solana/cosmos) |
| `export [file]` | Export session history to JSON |
| `clear` | Clear screen |
| `help` | Show help |
| `quit` | Exit |

### What the Inspector Shows

For each query, you'll see:

- **Confidence Meter**: Visual 0-100% bar (green ≥70%, yellow ≥50%, red <50%)
- **RAG Metrics**: Retrieval quality, sources used, query type, processing time
- **Corrective RAG Indicator**: Shows when retries were triggered and what alternative queries were tried
- **Search Guidance**: When docs are insufficient, shows suggested web searches
- **Suggestions**: Follow-up actions and related queries
- **Answer Preview**: First 500 characters of the response

### Special Modes

**Compare Mode** (`compare`):
Enter two queries and see a side-by-side comparison of confidence, quality, and whether search guidance was triggered.

**Stress Test** (`stress <topic>`):
Runs 8 variations of a query (e.g., "What is X?", "How do I use X?", "X example code") and shows aggregate statistics including average confidence and quality distribution.

**Export** (`export session.json`):
Saves full session history with raw responses for offline analysis.

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | (required) | OpenAI API key for embeddings and LLM |
| `MCP_PORT` | 3000 | Server port |
| `MCP_HOST` | localhost | Server host |
| `QDRANT_URL` | http://localhost:6333 | Qdrant URL |
| `QDRANT_COLLECTION` | crypto_docs | Qdrant collection name |
| `SQLITE_PATH` | ./data/crypto_docs.db | SQLite database path |
| `LLM_MODEL` | gpt-4o | OpenAI model for answer synthesis |
| `LLM_MAX_TOKENS` | 4000 | Maximum tokens for LLM responses |
| `LLM_TEMPERATURE` | 0.3 | LLM temperature (lower = more focused) |
| `GITHUB_TOKEN` | (optional) | GitHub token for higher API rate limits |

## How It Works

1. **Project Config** defines documentation URL, GitHub repo, and crawler settings
2. **Source Registry** (optional) defines multiple GitHub sources with trust levels
3. **Scraper** crawls the documentation site for a specific project
4. **Parser** converts HTML to structured chunks (prose, code, API reference)
5. **Intelligent GitHub Scraper** applies 4-stage quality filtering on code
6. **Chunker** splits large content with semantic overlap
7. **Embeddings** are generated via OpenAI text-embedding-3-small
8. **Qdrant** stores vectors for semantic search (with project tags)
9. **SQLite FTS5** provides fast full-text search (with project filtering)
10. **Hybrid Search** combines both using Reciprocal Rank Fusion
11. **Reranker** (gpt-4o-mini) scores top candidates for relevance
12. **Corrective RAG** retries with alternative queries if confidence is low
13. **LLM** (GPT-4o) synthesizes comprehensive answers with citations
14. **MCP Server** exposes tools via JSON-RPC over HTTP

## API Reference

### HTTP Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check with version info |
| `/mcp` | POST | MCP JSON-RPC 2.0 endpoint |
| `/mcp/events` | GET | Server-Sent Events (SSE) stream |

### Health Response

```json
{
  "status": "ok",
  "server": "crypto-docs-mcp",
  "version": "0.1.0",
  "features": ["llm-synthesis", "reranking"],
  "endpoints": {
    "mcp": "/mcp",
    "health": "/health"
  }
}
```

### MCP Methods

| Method | Description |
|--------|-------------|
| `initialize` | Initialize MCP connection |
| `tools/list` | List available tools |
| `tools/call` | Execute a tool |
| `resources/list` | List available resources |
| `resources/read` | Read a resource by URI |
| `ping` | Connection keepalive |

## Troubleshooting

### Qdrant Connection Failed
```bash
# Make sure Docker is running
docker-compose up -d

# Check Qdrant health
curl http://localhost:6333/health
```

### Empty Search Results
Run the scraper first to index documentation:
```bash
npm run scraper -- -p mina --use-registry
```

### OpenAI API Errors
Verify your API key is set correctly in `.env`:
```bash
echo $OPENAI_API_KEY
```

### "OPENAI_API_KEY environment variable is required"
The `.env` file must be present in the package directory being run. Copy it to both:
```bash
cp .env packages/scraper/.env
cp .env packages/server/.env
```

### "No projects configured"
Make sure project JSON files exist in `config/projects/` directory.

### LLM Synthesis Too Slow
- Reduce `LLM_MAX_TOKENS` for shorter responses
- Increase `LLM_TEMPERATURE` for faster (but less focused) generation
- Use `crypto_search_docs` tool for raw results without LLM processing

## License

MIT

## Resources

- [Model Context Protocol](https://modelcontextprotocol.io)
- [Mina Protocol Documentation](https://docs.minaprotocol.com)
- [Solana Documentation](https://solana.com/docs)
- [Cosmos SDK Documentation](https://docs.cosmos.network)
- [Secret Network Documentation](https://docs.scrt.network)
- [Beam Documentation](https://github.com/BeamMW/beam/wiki)
- [Pirate Chain Documentation](https://docs.piratechain.com)
