# =============================================================================
# API KEYS (at least one LLM provider required)
# =============================================================================

# OpenAI - for GPT models (also used for embeddings)
OPENAI_API_KEY=sk-...

# Anthropic - for Claude models
ANTHROPIC_API_KEY=sk-ant-...

# XAI - for Grok models
XAI_API_KEY=xai-...

# AWS Bedrock - for Kimi K2 and other Bedrock models
# Uses AWS credential chain (env vars, profiles, IAM roles)
# AWS_ACCESS_KEY_ID=...
# AWS_SECRET_ACCESS_KEY=...
# AWS_REGION=us-east-1

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================

# Default provider for all LLM calls (openai|anthropic|xai)
# Auto-detected from available API keys if not set
# LLM_PROVIDER=openai

# Per-purpose provider overrides (optional)
# LLM_SYNTHESIS_PROVIDER=openai
# LLM_EVALUATION_PROVIDER=openai
# LLM_REFINEMENT_PROVIDER=openai

# =============================================================================
# LLM MODEL CONFIGURATION
# =============================================================================

# Default model (uses provider default if not set)
# LLM_MODEL=gpt-4.1

# Per-purpose model overrides (RECOMMENDED - use smaller model for evaluation)
#
# OPENAI OPTIONS:
#   gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, gpt-4o, gpt-4o-mini, o3, o4-mini
#
# ANTHROPIC OPTIONS:
#   claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001, claude-opus-4-5-20251101
#   claude-sonnet-4-20250514, claude-opus-4-1-20250805
#
# XAI OPTIONS:
#   grok-3, grok-3-mini, grok-4, grok-4-fast-reasoning
#
# BEDROCK OPTIONS:
#   moonshot.kimi-k2-thinking (Kimi K2 with extended thinking)
#   anthropic.claude-3-5-sonnet-20241022-v2:0 (Claude on Bedrock)
#   amazon.nova-pro-v1:0 (Amazon Nova Pro)
#
# LLM_SYNTHESIS_MODEL=gpt-4.1
# LLM_EVALUATION_MODEL=gpt-4.1-mini
# LLM_REFINEMENT_MODEL=gpt-4.1

# LLM Parameters
# LLM_MAX_TOKENS=4000
# LLM_TEMPERATURE=0.3
# LLM_SYNTHESIS_MAX_TOKENS=4000
# LLM_EVALUATION_MAX_TOKENS=2000
# LLM_REFINEMENT_MAX_TOKENS=4000

# Web Result Analyzer (parallel analysis of search results)
# Uses a fast model to analyze web results in parallel before refinement
# LLM_ANALYZER_PROVIDER=openai
# LLM_ANALYZER_MODEL=gpt-4.1-mini    # Fast model for parallel analysis
# LLM_ANALYZER_MAX_TOKENS=1000
# LLM_ANALYZER_TEMPERATURE=0.1

# =============================================================================
# WEB SEARCH (Tavily - optional, enables web search in agentic loop)
# =============================================================================

# TAVILY_API_KEY=tvly-...
# TAVILY_SEARCH_DEPTH=basic
# TAVILY_MAX_RESULTS=5

# =============================================================================
# AGENTIC EVALUATION LOOP
# =============================================================================

# Enable/disable agentic evaluation (default: true)
# AGENTIC_EVALUATION_ENABLED=true

# Max evaluation iterations (default: 3)
# AGENTIC_MAX_ITERATIONS=3

# Confidence threshold to skip evaluation (default: 85)
# AGENTIC_AUTO_RETURN_THRESHOLD=85

# Max web searches per query (default: 2)
# AGENTIC_MAX_WEB_SEARCHES=2

# Max additional doc queries per evaluation (default: 2)
# AGENTIC_MAX_DOC_QUERIES=2

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================

# Qdrant Vector Database
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=crypto_docs

# SQLite Full-Text Search Database
SQLITE_PATH=./data/crypto_docs.db

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

MCP_PORT=3000
MCP_HOST=localhost

# =============================================================================
# OPTIONAL
# =============================================================================

# GitHub token for increased API rate limits when scraping
# GITHUB_TOKEN=ghp_...
